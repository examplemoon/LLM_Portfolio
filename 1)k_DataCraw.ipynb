{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f2466e86-0133-4010-a4cf-7ebd1e62da6b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파일이 Data\\kopsi_stocks.csv에 저장되었습니다.\n",
      "      Company     Code\n",
      "0        삼성전자  '005930\n",
      "1      SK하이닉스  '000660\n",
      "2    LG에너지솔루션  '373220\n",
      "3    삼성바이오로직스  '207940\n",
      "4         현대차  '005380\n",
      "5          기아  '000270\n",
      "6        셀트리온  '068270\n",
      "7        KB금융  '105560\n",
      "8       NAVER  '035420\n",
      "9        신한지주  '055550\n",
      "10      현대모비스  '012330\n",
      "11   POSCO홀딩스  '005490\n",
      "12       고려아연  '010130\n",
      "13       삼성물산  '028260\n",
      "14       LG화학  '051910\n",
      "15       삼성생명  '032830\n",
      "16    메리츠금융지주  '138040\n",
      "17    HD현대중공업  '329180\n",
      "18  한화에어로스페이스  '012450\n",
      "19     하나금융지주  '086790\n",
      "20      삼성SDI  '006400\n",
      "21        HMM  '011200\n",
      "22       삼성화재  '000810\n",
      "23        카카오  '035720\n",
      "24       KT&G  '033780\n",
      "25       한국전력  '015760\n",
      "26       LG전자  '066570\n",
      "27    두산에너빌리티  '034020\n",
      "28       크래프톤  '259960\n",
      "29   HD한국조선해양  '009540\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# URL 설정\n",
    "url = 'https://finance.naver.com/sise/sise_market_sum.nhn?&page='\n",
    "\n",
    "# 데이터 저장 리스트 및 카운터\n",
    "company_data = []\n",
    "count = 0\n",
    "\n",
    "# 페이지 별로 데이터 수집\n",
    "for page in range(1, 40):\n",
    "    res = requests.get(url + str(page))\n",
    "    html = res.content.decode('euc-kr', 'replace')\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    stock_table = soup.find('table', attrs={'class': 'type_2'})\n",
    "    stock_rows = stock_table.find_all('tr', onmouseover=True)\n",
    "\n",
    "    # 주식 데이터 파싱\n",
    "    for row in stock_rows:\n",
    "        if count >= 30:  # 기업 수를 10개로 제한\n",
    "            break\n",
    "\n",
    "        stock_data = row.find_all('td')\n",
    "        \n",
    "        if len(stock_data) > 1:\n",
    "            per = stock_data[11].text.strip()\n",
    "            if per != 'N/A':\n",
    "                company_name = stock_data[1].text.strip()\n",
    "                company_code = stock_data[1].find('a')['href'].split('code=')[-1]\n",
    "                company_code = \"'\" + company_code.zfill(6)\n",
    "\n",
    "                company_data.append([company_name, company_code])\n",
    "                count += 1\n",
    "    if count >= 10:  # 10개 수집 시 루프 종료\n",
    "        break\n",
    "\n",
    "# DataFrame으로 변환\n",
    "df = pd.DataFrame(data=company_data, columns=['Company', 'Code'])\n",
    "\n",
    "# Data 폴더 생성 (폴더가 없으면 생성)\n",
    "data_folder = 'Data'\n",
    "os.makedirs(data_folder, exist_ok=True)\n",
    "\n",
    "# CSV 파일 저장\n",
    "output_path = os.path.join(data_folder, 'kopsi_stocks.csv')\n",
    "df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"파일이 {output_path}에 저장되었습니다.\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "93a3b13a-744c-43ff-8050-be0fa29b7671",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모든 뉴스 수집이 완료되었습니다. 파일은 Data/news_data.csv에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import urllib.parse\n",
    "import os\n",
    "\n",
    "def collect_news_for_date(search_query, code, date):\n",
    "    news_list = []\n",
    "    encoded_query = urllib.parse.quote(search_query.encode('euc-kr'))\n",
    "    url = f\"https://finance.naver.com/news/news_search.naver?q={encoded_query}&sm=all.basic&pd=1&stDateStart={date}&stDateEnd={date}\"\n",
    "    response = requests.get(url)\n",
    "    response.encoding = 'euc-kr'\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    articles = soup.find_all('dd', class_='articleSubject')[:1]  # Limit to 1 news title per date\n",
    "    for article in articles:\n",
    "        title = article.find('a').text.strip()\n",
    "        news_list.append({'Date': date, 'Code': code, 'Company': search_query, 'Title': title})\n",
    "    return news_list\n",
    "\n",
    "def collect_and_save_all_news(df, start_date, end_date):\n",
    "    start_date = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "    end_date = datetime.strptime(end_date, '%Y-%m-%d')\n",
    "    \n",
    "    # 빈 데이터프레임 생성\n",
    "    result_df = pd.DataFrame(columns=['Date', 'Code', 'Company', 'Title'])\n",
    "\n",
    "    # 뉴스 데이터 수집\n",
    "    for _, row in df.iterrows():\n",
    "        company = row['Company']\n",
    "        code = row['Code']\n",
    "        current_date = start_date\n",
    "        while current_date <= end_date:\n",
    "            # 월, 화, 수, 목, 일(평일+일요일)만 수집\n",
    "            if current_date.weekday() in [0, 1, 2, 3, 6]:  # 월: 0, 화: 1, 수: 2, 목: 3, 일: 6\n",
    "                date_str = current_date.strftime('%Y-%m-%d')\n",
    "                news = collect_news_for_date(company, code, date_str)\n",
    "                if news:\n",
    "                    # 수집된 뉴스 데이터 추가\n",
    "                    result_df = pd.concat([result_df, pd.DataFrame(news)], ignore_index=True)\n",
    "            \n",
    "            # 다음 날짜로 이동\n",
    "            current_date += timedelta(days=1)\n",
    "    \n",
    "    # 데이터 저장\n",
    "    output_path = 'Data/news_data.csv'\n",
    "    os.makedirs('Data', exist_ok=True)\n",
    "    result_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    print(f\"모든 뉴스 수집이 완료되었습니다. 파일은 {output_path}에 저장되었습니다.\")\n",
    "\n",
    "# 주식 데이터 로드 및 뉴스 수집 실행\n",
    "df = pd.read_csv('Data/kopsi_stocks.csv')\n",
    "collect_and_save_all_news(df, '2021-01-01', '2023-12-31')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00866e3-cbf5-42ae-a923-9eecc6db990c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
